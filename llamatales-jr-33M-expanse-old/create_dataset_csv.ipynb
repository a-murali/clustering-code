{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cc00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/a3murali/.cache/huggingface/token\n",
      "Login successful\n",
      "PyTorch Version: 1.13.0a0+d321be6\n",
      "Transformers Version: 4.35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a dragon named Timmy. Timmy lived in a cave with his best friend, a little rabbit named Rosie. Timmy was a very happy dragon. He loved to play with his best friend.\n",
      "\n",
      "One day, Timmy's cave started to smell very bad. He could not see very well or breathe fire. Timmy was worried that he had been in trouble. He asked Rosie, \"What's wrong?\" Rosie looked at him and said, \"The dragon of smoke is coming out of your cave.\" Timmy didn't know what Rosie meant by \"naughty\" or \"nosy.\" But he felt a little scared.\n",
      "\n",
      "Timmy and Rosie decided to go outside and see what was happening. They looked through the cave entrance and saw smoke everywhere. They saw that all the trees were on fire. Timmy's heart was racing fast. He was worried his cave was going to get in trouble. Rosie said, \"Don't worry, Timmy. We'll get out of here.\" They walked carefully outside, away from the fire.\n",
      "\n",
      "As they walked out, they saw the villagers working together to clean up the fire. Timmy was relieved to be safe. He hugged Rosie and said, \"Thank you for helping me, Rosie.\" Rosie smiled and said, \"Anytime, Timmy.\" Timmy felt happy again. From then on, he knew that he could always ask Rosie for help when he needed it.\n",
      "\n",
      "The next day, Timmy and Rosie went on an adventure together. They saw the villagers working hard to keep their houses safe. Timmy said, \"I'm glad we're friends, Rosie. You always remind me that I have good friends who care about me.\" Rosie smiled and said, \"I'm glad we're friends too, Timmy.\"\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=128256, bias=False)\n",
      ")\n",
      "model.embed_tokens.weight -> cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.0.input_layernorm.weight -> cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.1.input_layernorm.weight -> cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.2.input_layernorm.weight -> cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.3.input_layernorm.weight -> cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.4.input_layernorm.weight -> cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.5.input_layernorm.weight -> cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.6.input_layernorm.weight -> cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.7.input_layernorm.weight -> cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight -> cuda:0\n",
      "model.norm.weight -> cuda:0\n",
      "lm_head.weight -> cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (512). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.generation.utils.SampleDecoderOnlyOutput'>\n",
      "512\n",
      "9\n",
      "1\n",
      "365\n",
      "512\n",
      "Once upon a time there was a dragon named Firedog. Firedog loved to play outside in the sunshine. He would run and jump and roar loudly, \"Roar!\" to make all the children in the village wake up.\n",
      "\n",
      "Firedog lived in a big cave with a friendly owner named Lily. Lily took good care of Firedog by giving him yummy food and a fresh drink of water every day. She was a very dependable friend to Firedog. But one day, a big storm came to the village. The strong wind knocked over a big tree, making a big crash in the cave. Lily tried to move the tree but it was too heavy.\n",
      "\n",
      "The storm got stronger and stronger, and the village got messy. Fierce winds knocked over big things, like the roof and the walls. The villagers cried out, \"Help! Help!\" But Fog was too weak to help. He got stuck in the falling tree branches. The villagers tried to get him out, but it was too late. Fierce winds had taken Farks from the village and they were lost. The villagers were very sad and cried for Fierce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.generation.utils.SampleDecoderOnlyOutput'>\n",
      "512\n",
      "9\n",
      "1\n",
      "229\n",
      "512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>story</th>\n",
       "      <th>hidden_states</th>\n",
       "      <th>output_token_prompt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time there was a dragon</td>\n",
       "      <td>Once upon a time there was a dragon named Timm...</td>\n",
       "      <td>[[[[[ 0.01972523  0.02009857  0.02478487 ...  ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time there was a dragon</td>\n",
       "      <td>Once upon a time there was a dragon named Fire...</td>\n",
       "      <td>[[[[[ 0.02893161 -0.02468858 -0.01625257 ...  ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  \\\n",
       "0  Once upon a time there was a dragon   \n",
       "1  Once upon a time there was a dragon   \n",
       "\n",
       "                                               story  \\\n",
       "0  Once upon a time there was a dragon named Timm...   \n",
       "1  Once upon a time there was a dragon named Fire...   \n",
       "\n",
       "                                       hidden_states  \\\n",
       "0  [[[[[ 0.01972523  0.02009857  0.02478487 ...  ...   \n",
       "1  [[[[[ 0.02893161 -0.02468858 -0.01625257 ...  ...   \n",
       "\n",
       "                              output_token_prompt_id  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/expanse/lustre/projects/csd819/a3murali/cache/'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"<hf_token>\")\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM, pipeline, LlamaForCausalLM\n",
    "\n",
    "model = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "\n",
    "llamatales_pipeline = pipeline(\"text-generation\", model = model, device = \"cuda\")\n",
    "\n",
    "# prompts = {\n",
    "#     1: \"Once upon a time there was a dragon\", \n",
    "#     2: \"Once upon a time there was a princess\", \n",
    "#     3: \"Once upon a time there were two children\",\n",
    "#     4: \"Once upon a time there was a prince\",\n",
    "#     5: \"Once upon a time there was a frog\",\n",
    "#     6: \"Once upon a time there was a king\",\n",
    "#     7: \"Once upon a time there was a queen\",\n",
    "#     8: \"Once upon a time there was a wolf\",\n",
    "#     9: \"Once upon a time there was a genie\",\n",
    "#     10: \"Once upon a time there was a poor boy\"\n",
    "# }\n",
    "prompts = {1: \"Once upon a time there was a dragon\"}\n",
    "\n",
    "data = []\n",
    "for prompt_id in prompts:\n",
    "    #generation - generate stories for each prompt\n",
    "    for i in range(2):\n",
    "        sequences = llamatales_pipeline(\n",
    "            prompts[prompt_id],\n",
    "            do_sample = True,\n",
    "            top_k = 10,\n",
    "            num_return_sequences = 1,\n",
    "            max_new_tokens = 512\n",
    "        )\n",
    "        generated_story = sequences[0]['generated_text']\n",
    "        print(generated_story)\n",
    "\n",
    "        num_tokens_generated_story = len(tokenizer.encode(generated_story))\n",
    "\n",
    "        #test - collect hidden states\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\"/expanse/lustre/projects/csd819/a3murali/cache/models--ivnle--llamatales_jr_8b-lay8-hs512-hd8-33M/snapshots/461f50f0024efb46b94dc68cc850d12d75ecb325\", output_hidden_states = True)\n",
    "        model = AutoModelForCausalLM.from_config(config).to('cuda')\n",
    "        if(i == 0):\n",
    "            print(model)\n",
    "            \n",
    "            for j in model.named_parameters():\n",
    "                print(f\"{j[0]} -> {j[1].device}\")\n",
    "\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"/expanse/lustre/projects/csd819/a3murali/cache/models--ivnle--llamatales_jr_8b-lay8-hs512-hd8-33M/snapshots/461f50f0024efb46b94dc68cc850d12d75ecb325/\", config = config)\n",
    "\n",
    "        inputs = tokenizer(generated_story, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(inputs.input_ids, attention_mask=inputs[\"attention_mask\"], do_sample = True, top_k = 10, num_return_sequences = 1, max_new_tokens = 512, eos_token_id = tokenizer.eos_token_id, pad_token_id = tokenizer.pad_token_id, return_dict_in_generate=True)\n",
    "        \n",
    "        #analysis - collect prompt id of each token\n",
    "        output_id = []\n",
    "        for token in range(num_tokens_generated_story):\n",
    "            output_id.append(prompt_id)\n",
    "        \n",
    "        convert_hidden_states = []\n",
    "        print(type(outputs))\n",
    "\n",
    "        for i in outputs.hidden_states:\n",
    "            convert_hidden_states.append([j.detach().cpu().numpy() for j in i])\n",
    "        \n",
    "        print(len(convert_hidden_states))\n",
    "        print(len(convert_hidden_states[0]))\n",
    "        print(len(convert_hidden_states[0][0]))\n",
    "        print(len(convert_hidden_states[0][0][0]))\n",
    "        print(len(convert_hidden_states[0][0][0][0]))\n",
    "        #print(len(convert_hidden_states[0][0][0][0][0]))\n",
    "        data.append([prompts[prompt_id], generated_story, convert_hidden_states, np.array(output_id)])\n",
    "\n",
    "        #(([]))\n",
    "\n",
    "df = pd.DataFrame(data, columns = [\"prompt\", \"story\", \"hidden_states\", \"output_token_prompt_id\"], dtype = 'object')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ae83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"story_dataset.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
