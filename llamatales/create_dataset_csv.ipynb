{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b169aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.7.0+cu126\n",
      "Transformers Version: 4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'prefill_chunk_size': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 128000, 'eos_token_id': 128009, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, 'generation_kwargs': {}, '_from_model_config': True, 'transformers_version': '4.52.4'}\n",
      "Once upon a time there was a dragon. The dragon's name was Max. Max loved to play outside in the sunshine. One day, Max saw a beautiful red wagon. The wagon was tied to a tree.\n",
      "\n",
      "Max wanted to play with the wagon. He wanted to ride in it. But the wagon belonged to a grumpy old man. The man said, \"No, Max. You cannot play with my wagon.\" Max was sad. He wanted to ride in the wagon so badly.\n",
      "\n",
      "Max decided to take the wagon anyway. He pulled and pulled until the wagon was in the air. He was so happy! But then, Max got too excited. He pulled the wagon too hard. The wagon fell down and broke.\n",
      "\n",
      "Max was very sad. He had broken the wagon. The grumpy old man was very angry. He said, \"You broke my wagon!\" Max said, \"I'm sorry.\" The grumpy old man said, \"You have to fix the wagon, or I will not let you play in it anymore.\" Max was very sad. He had to say goodbye to the wagon.\n",
      "Input Shape:  224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden States Type:  <class 'tuple'>\n",
      "Hidden States Length:  512\n",
      "Hidden States Shape:  torch.Size([1, 736])\n",
      "512\n",
      "9\n",
      "1\n",
      "224\n",
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'prefill_chunk_size': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 128000, 'eos_token_id': 128009, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, 'generation_kwargs': {}, '_from_model_config': True, 'transformers_version': '4.52.4'}\n",
      "Once upon a time there was a dragon named Scorch. Scorch loved to play outside in the sunshine. He would run and jump, and breathe fire. The people in the village thought Scorch was very cool.\n",
      "\n",
      "One day, Scorch found a little bird lying on the ground. The bird's wing was hurt, and it couldn't fly. Scorch knew he had to help the bird. He gently picked it up and took it to his cave. The bird's name was Chirpy, and Chirpy's feathers were dirty from flying. Scorch gave Chirpy a bath, and then he used a soft brush to clean Chirpy's feathers.\n",
      "\n",
      "After Chirpy's feathers were clean, Scorch took it outside to play again. But, oh no! Scorch forgot to put a pin on the back of his collar. When he ran, he tripped and fell. The pin went flying off! Scorch felt very sad. He thought about how much he missed his new friend Chirpy.\n",
      "\n",
      "But then, Scorch remembered what his momma dragon told him: \"When we help others, we feel happy and proud of ourselves.\" Scorch decided to go back to his cave and find the pin. He looked high and low until he found it, and he put it on the bird's wing. The bird was happy, and Scorch was happy too! From that day on, Scorch remembered to always help others. He learned that being kind and helping others makes everyone happy.\n",
      "Input Shape:  310\n",
      "Hidden States Type:  <class 'tuple'>\n",
      "Hidden States Length:  512\n",
      "Hidden States Shape:  torch.Size([1, 822])\n",
      "512\n",
      "9\n",
      "1\n",
      "310\n",
      "512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>story</th>\n",
       "      <th>hidden_states</th>\n",
       "      <th>output_token_prompt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time there was a dragon</td>\n",
       "      <td>Once upon a time there was a dragon. The drago...</td>\n",
       "      <td>[[[[[ 0.00965552  0.0096138  -0.0381562  ... -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time there was a dragon</td>\n",
       "      <td>Once upon a time there was a dragon named Scor...</td>\n",
       "      <td>[[[[[-0.00458247 -0.02922091 -0.00169429 ...  ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  \\\n",
       "0  Once upon a time there was a dragon   \n",
       "1  Once upon a time there was a dragon   \n",
       "\n",
       "                                               story  \\\n",
       "0  Once upon a time there was a dragon. The drago...   \n",
       "1  Once upon a time there was a dragon named Scor...   \n",
       "\n",
       "                                       hidden_states  \\\n",
       "0  [[[[[ 0.00965552  0.0096138  -0.0381562  ... -...   \n",
       "1  [[[[[-0.00458247 -0.02922091 -0.00169429 ...  ...   \n",
       "\n",
       "                              output_token_prompt_id  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/expanse/lustre/projects/csd819/a3murali/cache/'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"<hf_token>\")\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM, pipeline, LlamaForCausalLM\n",
    "\n",
    "model = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "\n",
    "llamatales_pipeline = pipeline(\"text-generation\", model = model, device = \"cuda\")\n",
    "\n",
    "# prompts = {\n",
    "#     1: \"Once upon a time there was a dragon\", \n",
    "#     2: \"Once upon a time there was a princess\", \n",
    "#     3: \"Once upon a time there were two children\",\n",
    "#     4: \"Once upon a time there was a prince\",\n",
    "#     5: \"Once upon a time there was a frog\",\n",
    "#     6: \"Once upon a time there was a king\",\n",
    "#     7: \"Once upon a time there was a queen\",\n",
    "#     8: \"Once upon a time there was a wolf\",\n",
    "#     9: \"Once upon a time there was a genie\",\n",
    "#     10: \"Once upon a time there was a poor boy\"\n",
    "# }\n",
    "prompts = {1: \"Once upon a time there was a dragon\"}\n",
    "\n",
    "data = []\n",
    "for prompt_id in prompts:\n",
    "    #generation - generate stories for each prompt\n",
    "    for i in range(2):\n",
    "        sequences = llamatales_pipeline(\n",
    "            prompts[prompt_id],\n",
    "            do_sample = True,\n",
    "            top_k = 10,\n",
    "            num_return_sequences = 1,\n",
    "            max_new_tokens = 512,\n",
    "        )\n",
    "\n",
    "        print(llamatales_pipeline.model.config)\n",
    "        print(llamatales_pipeline.model.generation_config.to_dict())\n",
    "        \n",
    "        generated_story = sequences[0]['generated_text']\n",
    "        print(generated_story)\n",
    "\n",
    "        num_tokens_generated_story = len(tokenizer.encode(generated_story))\n",
    "\n",
    "        #test - collect hidden states\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\"/expanse/lustre/projects/csd819/a3murali/cache/models--ivnle--llamatales_jr_8b-lay8-hs512-hd8-33M/snapshots/461f50f0024efb46b94dc68cc850d12d75ecb325\", output_hidden_states = True)\n",
    "        model = AutoModelForCausalLM.from_config(config).to('cuda')\n",
    "        # if(i == 0):\n",
    "        #     print(model)\n",
    "            \n",
    "        #     for j in model.named_parameters():\n",
    "        #         print(f\"{j[0]} -> {j[1].device}\")\n",
    "\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"/expanse/lustre/projects/csd819/a3murali/cache/models--ivnle--llamatales_jr_8b-lay8-hs512-hd8-33M/snapshots/461f50f0024efb46b94dc68cc850d12d75ecb325/\", config = config)\n",
    "\n",
    "        inputs = tokenizer(generated_story, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        print(\"Input Shape: \", inputs.input_ids.shape[1])\n",
    "\n",
    "        outputs = model.generate(inputs.input_ids, attention_mask=inputs[\"attention_mask\"], do_sample = True, top_k = 10, num_return_sequences = 1, max_new_tokens = 512, eos_token_id = tokenizer.eos_token_id, pad_token_id = tokenizer.pad_token_id, return_dict_in_generate=True)\n",
    "        \n",
    "        #analysis - collect prompt id of each token\n",
    "        output_id = []\n",
    "        for token in range(num_tokens_generated_story):\n",
    "            output_id.append(prompt_id)\n",
    "        \n",
    "        convert_hidden_states = []\n",
    "        print(\"Hidden States Type: \", type(outputs.hidden_states))\n",
    "        print(\"Hidden States Length: \", len(outputs.hidden_states))\n",
    "        print(\"Hidden States Shape: \", outputs.sequences.shape)\n",
    "\n",
    "        for i in outputs.hidden_states:\n",
    "            convert_hidden_states.append([j.detach().cpu().numpy() for j in i])\n",
    "        \n",
    "        print(len(convert_hidden_states))\n",
    "        print(len(convert_hidden_states[0]))\n",
    "        print(len(convert_hidden_states[0][0]))\n",
    "        print(len(convert_hidden_states[0][0][0]))\n",
    "        print(len(convert_hidden_states[0][0][0][0]))\n",
    "        # print(len(convert_hidden_states[0][0][0][0][0]))\n",
    "        data.append([prompts[prompt_id], generated_story, convert_hidden_states, np.array(output_id)])\n",
    "\n",
    "        #(([]))\n",
    "\n",
    "df = pd.DataFrame(data, columns = [\"prompt\", \"story\", \"hidden_states\", \"output_token_prompt_id\"], dtype = 'object')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802f2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"story_dataset.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
