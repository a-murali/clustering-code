{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacd4f45-6e41-4199-9b4a-7df3cba847e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/sbksvol/amurali/'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"<hf_token>\")\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM, pipeline, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a347c0da-c00f-41be-a0eb-084dfcb20092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.2+cu121\n",
      "Transformers Version: 4.33.3\n",
      "NumPy Version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ad3c1b-baa2-47f7-929f-8936c636b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb67a3e6-323f-45eb-a0b9-7e625c5ec3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=128256, bias=False)\n",
      ")\n",
      "model.embed_tokens.weight -> cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.0.input_layernorm.weight -> cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.1.input_layernorm.weight -> cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.2.input_layernorm.weight -> cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.3.input_layernorm.weight -> cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.4.input_layernorm.weight -> cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.5.input_layernorm.weight -> cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.6.input_layernorm.weight -> cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.7.input_layernorm.weight -> cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight -> cuda:0\n",
      "model.norm.weight -> cuda:0\n",
      "lm_head.weight -> cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "    \n",
    "for j in model.named_parameters():\n",
    "    print(f\"{j[0]} -> {j[1].device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4e9f2-5f13-4e96-b866-dd1daba8e719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
