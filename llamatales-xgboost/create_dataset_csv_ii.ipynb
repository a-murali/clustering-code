{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f78858-6116-4b7c-b63c-e84194d74685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get hidden states by context level and by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c08f418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/sbksvol/amurali/'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"<hf_token>\")\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM, pipeline, LlamaForCausalLM\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194dfcce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.2+cu121\n",
      "Transformers Version: 4.33.3\n",
      "NumPy Version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6337c21e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: \"Once upon a time there was a dragon\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1000 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n",
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n",
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n",
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n",
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n",
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 1/1000 [00:00<10:52,  1.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n",
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n",
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n",
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n",
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n",
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 2/1000 [00:01<10:46,  1.54it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n",
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n",
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n",
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n",
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n",
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                 | 3/1000 [00:01<10:43,  1.55it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n",
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n",
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n",
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n",
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n",
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 4/1000 [00:02<10:42,  1.55it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n",
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n",
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n",
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n",
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n",
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 5/1000 [00:03<10:39,  1.55it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n",
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n",
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n",
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n",
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n",
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                 | 6/1000 [00:03<10:37,  1.56it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n",
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n",
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n",
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n",
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n",
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n",
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n",
      "2\n",
      "9\n",
      "(1, 9, 512)\n",
      "3\n",
      "9\n",
      "(1, 9, 512)\n",
      "4\n",
      "9\n",
      "(1, 9, 512)\n",
      "5\n",
      "9\n",
      "(1, 9, 512)\n",
      "6\n",
      "9\n",
      "(1, 9, 512)\n",
      "7\n",
      "9\n",
      "(1, 9, 512)\n",
      "8\n",
      "9\n",
      "(1, 9, 512)\n",
      "9\n",
      "9\n",
      "(1, 9, 512)\n",
      "10\n",
      "9\n",
      "(1, 9, 512)\n",
      "11\n",
      "9\n",
      "(1, 9, 512)\n",
      "12\n",
      "9\n",
      "(1, 9, 512)\n",
      "13\n",
      "9\n",
      "(1, 9, 512)\n",
      "14\n",
      "9\n",
      "(1, 9, 512)\n",
      "15\n",
      "9\n",
      "(1, 9, 512)\n",
      "16\n",
      "9\n",
      "(1, 9, 512)\n",
      "17\n",
      "9\n",
      "(1, 9, 512)\n",
      "18\n",
      "9\n",
      "(1, 9, 512)\n",
      "19\n",
      "9\n",
      "(1, 9, 512)\n",
      "20\n",
      "9\n",
      "(1, 9, 512)\n",
      "21\n",
      "9\n",
      "(1, 9, 512)\n",
      "22\n",
      "9\n",
      "(1, 9, 512)\n",
      "23\n",
      "9\n",
      "(1, 9, 512)\n",
      "24\n",
      "9\n",
      "(1, 9, 512)\n",
      "25\n",
      "9\n",
      "(1, 9, 512)\n",
      "26\n",
      "9\n",
      "(1, 9, 512)\n",
      "27\n",
      "9\n",
      "(1, 9, 512)\n",
      "28\n",
      "9\n",
      "(1, 9, 512)\n",
      "29\n",
      "9\n",
      "(1, 9, 512)\n",
      "30\n",
      "9\n",
      "(1, 9, 512)\n",
      "31\n",
      "9\n",
      "(1, 9, 512)\n",
      "32\n",
      "9\n",
      "(1, 9, 512)\n",
      "33\n",
      "9\n",
      "(1, 9, 512)\n",
      "34\n",
      "9\n",
      "(1, 9, 512)\n",
      "35\n",
      "9\n",
      "(1, 9, 512)\n",
      "36\n",
      "9\n",
      "(1, 9, 512)\n",
      "37\n",
      "9\n",
      "(1, 9, 512)\n",
      "38\n",
      "9\n",
      "(1, 9, 512)\n",
      "39\n",
      "9\n",
      "(1, 9, 512)\n",
      "40\n",
      "9\n",
      "(1, 9, 512)\n",
      "41\n",
      "9\n",
      "(1, 9, 512)\n",
      "42\n",
      "9\n",
      "(1, 9, 512)\n",
      "43\n",
      "9\n",
      "(1, 9, 512)\n",
      "44\n",
      "9\n",
      "(1, 9, 512)\n",
      "45\n",
      "9\n",
      "(1, 9, 512)\n",
      "46\n",
      "9\n",
      "(1, 9, 512)\n",
      "47\n",
      "9\n",
      "(1, 9, 512)\n",
      "48\n",
      "9\n",
      "(1, 9, 512)\n",
      "49\n",
      "9\n",
      "(1, 9, 512)\n",
      "50\n",
      "9\n",
      "(1, 9, 512)\n",
      "51\n",
      "9\n",
      "(1, 9, 512)\n",
      "52\n",
      "9\n",
      "(1, 9, 512)\n",
      "53\n",
      "9\n",
      "(1, 9, 512)\n",
      "54\n",
      "9\n",
      "(1, 9, 512)\n",
      "55\n",
      "9\n",
      "(1, 9, 512)\n",
      "56\n",
      "9\n",
      "(1, 9, 512)\n",
      "57\n",
      "9\n",
      "(1, 9, 512)\n",
      "58\n",
      "9\n",
      "(1, 9, 512)\n",
      "59\n",
      "9\n",
      "(1, 9, 512)\n",
      "60\n",
      "9\n",
      "(1, 9, 512)\n",
      "61\n",
      "9\n",
      "(1, 9, 512)\n",
      "62\n",
      "9\n",
      "(1, 9, 512)\n",
      "63\n",
      "9\n",
      "(1, 9, 512)\n",
      "64\n",
      "9\n",
      "(1, 9, 512)\n",
      "65\n",
      "9\n",
      "(1, 9, 512)\n",
      "66\n",
      "9\n",
      "(1, 9, 512)\n",
      "67\n",
      "9\n",
      "(1, 9, 512)\n",
      "68\n",
      "9\n",
      "(1, 9, 512)\n",
      "69\n",
      "9\n",
      "(1, 9, 512)\n",
      "70\n",
      "9\n",
      "(1, 9, 512)\n",
      "71\n",
      "9\n",
      "(1, 9, 512)\n",
      "72\n",
      "9\n",
      "(1, 9, 512)\n",
      "73\n",
      "9\n",
      "(1, 9, 512)\n",
      "74\n",
      "9\n",
      "(1, 9, 512)\n",
      "75\n",
      "9\n",
      "(1, 9, 512)\n",
      "76\n",
      "9\n",
      "(1, 9, 512)\n",
      "77\n",
      "9\n",
      "(1, 9, 512)\n",
      "78\n",
      "9\n",
      "(1, 9, 512)\n",
      "79\n",
      "9\n",
      "(1, 9, 512)\n",
      "80\n",
      "9\n",
      "(1, 9, 512)\n",
      "81\n",
      "9\n",
      "(1, 9, 512)\n",
      "82\n",
      "9\n",
      "(1, 9, 512)\n",
      "83\n",
      "9\n",
      "(1, 9, 512)\n",
      "84\n",
      "9\n",
      "(1, 9, 512)\n",
      "85\n",
      "9\n",
      "(1, 9, 512)\n",
      "86\n",
      "9\n",
      "(1, 9, 512)\n",
      "87\n",
      "9\n",
      "(1, 9, 512)\n",
      "88\n",
      "9\n",
      "(1, 9, 512)\n",
      "89\n",
      "9\n",
      "(1, 9, 512)\n",
      "90\n",
      "9\n",
      "(1, 9, 512)\n",
      "91\n",
      "9\n",
      "(1, 9, 512)\n",
      "92\n",
      "9\n",
      "(1, 9, 512)\n",
      "93\n",
      "9\n",
      "(1, 9, 512)\n",
      "94\n",
      "9\n",
      "(1, 9, 512)\n",
      "95\n",
      "9\n",
      "(1, 9, 512)\n",
      "96\n",
      "9\n",
      "(1, 9, 512)\n",
      "97\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 7/1000 [00:04<10:37,  1.56it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "  1%|▌                                                                                 | 7/1000 [00:04<10:40,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "9\n",
      "(1, 9, 512)\n",
      "99\n",
      "9\n",
      "(1, 9, 512)\n",
      "100\n",
      "9\n",
      "(1, 9, 512)\n",
      "1\n",
      "9\n",
      "(1, 9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m hook \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mregister_forward_hook(capture_hidden_states)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Generate tokens\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_cache=False\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m hook\u001b[38;5;241m.\u001b[39mremove()  \u001b[38;5;66;03m# remove hook after generation\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Decode story\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2731\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:424\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:321\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    318\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(value_states, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    323\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load model/create pipeline\n",
    "# model_name = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(\"cuda\")\n",
    "# # llamatales_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# # Prompts\n",
    "# prompts = {\n",
    "#     1: \"Once upon a time there was a dragon\", \n",
    "#     2: \"Once upon a time there was a princess\", \n",
    "#     3: \"Once upon a time there were two children\",\n",
    "#     4: \"Once upon a time there was a prince\",\n",
    "#     5: \"Once upon a time there was a frog\",\n",
    "#     6: \"Once upon a time there was a king\",\n",
    "#     7: \"Once upon a time there was a queen\",\n",
    "#     8: \"Once upon a time there was a wolf\",\n",
    "#     9: \"Once upon a time there was a genie\",\n",
    "#     10: \"Once upon a time there was a poor boy\"\n",
    "# }\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "\n",
    "# max_new_tokens = 100\n",
    "# top_k = 10\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# def capture_hidden_states(module, input, output):\n",
    "#     # output.hidden_states: tuple of (1, seq_len, hidden_dim)\n",
    "#     # store as CPU numpy\n",
    "#     hidden_states_storage.append([h.detach().cpu().numpy() for h in output.hidden_states])\n",
    "#     print(len(hidden_states_storage))\n",
    "#     print(len(hidden_states_storage[0]))\n",
    "#     print(hidden_states_storage[0][0].shape)\n",
    "                   \n",
    "# for prompt_id, prompt_text in prompts.items():\n",
    "#     print(f\"Prompt {prompt_id}: \\\"{prompt_text}\\\"\")\n",
    "#     data = []\n",
    "#     npz_data = {}\n",
    "#     hidden_state_file = f'./hidden_states/prompt_{prompt_id}.npz'\n",
    "\n",
    "#     # Encode prompt once\n",
    "#     enc = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "#     input_ids = enc.input_ids\n",
    "#     eos_id = tokenizer.eos_token_id\n",
    "\n",
    "#     for i in tqdm(range(1000)):\n",
    "#         hidden_states_storage = []\n",
    "\n",
    "#         # Register forward hook to capture hidden states\n",
    "#         hook = model.register_forward_hook(capture_hidden_states)\n",
    "\n",
    "#         # Generate tokens\n",
    "#         outputs = model.generate(\n",
    "#             input_ids=input_ids,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             do_sample=True,\n",
    "#             top_k=top_k,\n",
    "#             eos_token_id=eos_id,\n",
    "#             return_dict_in_generate=True,\n",
    "#             output_hidden_states=True,\n",
    "#             # use_cache=False\n",
    "#         )\n",
    "\n",
    "#         hook.remove()  # remove hook after generation\n",
    "\n",
    "#         # Decode story\n",
    "#         generated_ids = outputs.sequences\n",
    "#         generated_story = tokenizer.decode(\n",
    "#             generated_ids[0],\n",
    "#             skip_special_tokens=True,\n",
    "#             clean_up_tokenization_spaces=True\n",
    "#         )\n",
    "\n",
    "#         # Store hidden states safely\n",
    "#         wrapper = np.empty(1, dtype=object)\n",
    "#         wrapper[0] = hidden_states_storage\n",
    "#         npz_data[f\"arr_{i}\"] = wrapper\n",
    "\n",
    "#         data.append([\n",
    "#             prompt_id,\n",
    "#             prompt_text,\n",
    "#             generated_story,\n",
    "#             hidden_state_file,\n",
    "#             generated_ids.shape[1]\n",
    "#         ])\n",
    "\n",
    "#     # Save hidden states\n",
    "#     np.savez_compressed(hidden_state_file, **npz_data)\n",
    "\n",
    "#     # Append CSV\n",
    "#     df = pd.DataFrame(data, columns=[\"prompt_id\", \"prompt\", \"story\",\n",
    "#                                      \"hidden_state_file\", \"len_generated_story\"])\n",
    "#     if prompt_id == 1:\n",
    "#         df.to_csv(\"story_dataset_new.csv\", index=False)\n",
    "#     else:\n",
    "#         df.to_csv(\"story_dataset_new.csv\", mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4176d-39d1-44ee-b202-fb0bfa512b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model/create pipeline\n",
    "model_name = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(\"cuda\")\n",
    "\n",
    "# Prompts\n",
    "prompts = {\n",
    "    1: \"Once upon a time there was a dragon\", \n",
    "    2: \"Once upon a time there was a princess\", \n",
    "    3: \"Once upon a time there were two children\",\n",
    "    4: \"Once upon a time there was a prince\",\n",
    "    5: \"Once upon a time there was a frog\",\n",
    "    6: \"Once upon a time there was a king\",\n",
    "    7: \"Once upon a time there was a queen\",\n",
    "    8: \"Once upon a time there was a wolf\",\n",
    "    9: \"Once upon a time there was a genie\",\n",
    "    10: \"Once upon a time there was a poor boy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a132fda2-9475-4205-863a-4763ef0d904d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts.values():\n",
    "    print(len(tokenizer.encode(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f5ff0-e577-4c08-98a5-0a85f8ada247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     if(len(npz_data[f\"arr_{i}\"][0][0][0][0]) != 512):\n",
    "#         # print(i)\n",
    "#         print(len(npz_data[f\"arr_{i}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e350f-db30-47d5-8108-2c22889d1792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# npz_data[\"arr_1\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74ce8e-5c0d-46ff-8913-b6edff862302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#100x9x1x9x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03cda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"story_dataset_new.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
