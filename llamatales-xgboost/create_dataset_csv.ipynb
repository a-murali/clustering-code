{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f78858-6116-4b7c-b63c-e84194d74685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get hidden states by context level and by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c08f418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /sbksvol/amurali/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/sbksvol/amurali/'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"<hf_token>\")\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM, pipeline, LlamaForCausalLM\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194dfcce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.13.1+cu117\n",
      "Transformers Version: 4.30.2\n",
      "NumPy Version: 1.21.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6337c21e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: \"Once upon a time there was a dragon\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py:1260: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  \"You have modified the pretrained model configuration to control generation. This is a\"\n",
      "  1%|▊                                                                                        | 9/1000 [00:23<43:19,  2.62s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_330/2441109779.py\", line 54, in <module>\n",
      "    output_hidden_states=True\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\", line 1583, in generate\n",
      "    **model_kwargs,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\", line 2619, in sample\n",
      "    outputs = self(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/opt/conda/lib/python3.7/posixpath.py\", line 89, in join\n",
      "    elif not path or path.endswith(sep):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_330/2441109779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1582\u001b[0m                 \u001b[0mstreamer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2618\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2619\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2620\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2067\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Load model/create pipeline\n",
    "model_name = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(\"cuda\")\n",
    "# llamatales_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Prompts\n",
    "prompts = {\n",
    "    1: \"Once upon a time there was a dragon\", \n",
    "    2: \"Once upon a time there was a princess\", \n",
    "    3: \"Once upon a time there were two children\",\n",
    "    4: \"Once upon a time there was a prince\",\n",
    "    5: \"Once upon a time there was a frog\",\n",
    "    6: \"Once upon a time there was a king\",\n",
    "    7: \"Once upon a time there was a queen\",\n",
    "    8: \"Once upon a time there was a wolf\",\n",
    "    9: \"Once upon a time there was a genie\",\n",
    "    10: \"Once upon a time there was a poor boy\"\n",
    "}\n",
    "\n",
    "\n",
    "for prompt_id, prompt_text in prompts.items():\n",
    "    print(f\"Prompt {prompt_id}: \\\"{prompt_text}\\\"\")\n",
    "    data = []\n",
    "    npz_data = {}\n",
    "    hidden_state_file = f'./hidden_states/prompt_{prompt_id}.npz'\n",
    "    for i in tqdm(range(1000)):\n",
    "        # Generate using pipeline\n",
    "        # sequences = llamatales_pipeline(\n",
    "        #     prompt_text,\n",
    "        #     do_sample=True,\n",
    "        #     top_k=10,\n",
    "        #     num_return_sequences=1,\n",
    "        #     max_new_tokens=100\n",
    "        # )\n",
    "        # generated_story = sequences[0]['generated_text']\n",
    "#         print(f\"\\nGenerated (pipeline):\\n{generated_story}\")\n",
    "\n",
    "        # Count tokens\n",
    "        # num_tokens_generated_story = len(tokenizer.encode(generated_story))\n",
    "\n",
    "        # Generate hidden states from model.generate using same prompt\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        generated_story = tokenizer.batch_decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "#         print(\"New Story: \\n\", new_story)\n",
    "        \n",
    "#         print(\"Length of Generated Story: \", num_tokens_generated_story)\n",
    "#         print(\"Length of New Story: \", len(tokenizer.encode(new_story)))\n",
    "        \n",
    "        # Map output tokens to prompt ID\n",
    "        # output_id = [prompt_id] * num_tokens_generated_story\n",
    "\n",
    "        # Extract hidden states\n",
    "        convert_hidden_states = []\n",
    "#         print(f\"Type of hidden_states: {type(outputs.hidden_states)}\")\n",
    "        for layer_hidden_states in outputs.hidden_states:\n",
    "            convert_hidden_states.append([t.detach().cpu().numpy() for t in layer_hidden_states])\n",
    "\n",
    "        # Debug hidden state shape\n",
    "#         print(f\"Hidden states shape: {len(convert_hidden_states)} generated tokens × \"\n",
    "#               f\"{len(convert_hidden_states[0])} layers × \"\n",
    "#               f\"{len(convert_hidden_states[0][0])} sequences × \"\n",
    "#               f\"{len(convert_hidden_states[0][0][0])} tokens × \" \n",
    "#               f\"{len(convert_hidden_states[0][0][0][0])} dims\")\n",
    "\n",
    "#         print(\"Num Tokens: \", len(convert_hidden_states[0][0][0])) #same as context (length of original generated story)\n",
    "\n",
    "        # Save hidden states to file\n",
    "        arr = np.empty(len(convert_hidden_states), dtype=object)\n",
    "        arr[:] = convert_hidden_states\n",
    "        npz_data[f\"arr_{i}\"] = arr\n",
    "        # npz_data[f\"arr_{i}\"] = convert_hidden_states\n",
    "        \n",
    "        num_tokens_generated_story = len(tokenizer.encode(generated_story))\n",
    "        \n",
    "        # Store results\n",
    "        data.append([prompt_id, prompt_text, generated_story, hidden_state_file, num_tokens_generated_story])\n",
    "    \n",
    "    np.savez_compressed(hidden_state_file, **npz_data)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"prompt_id\", \"prompt\", \"story\", \"hidden_state_file\", \"len_generated_story\"])\n",
    "    if(prompt_id == 1): df.to_csv(\"story_dataset_new.csv\", index = False)\n",
    "    else:\n",
    "        df.to_csv('story_dataset_new.csv', mode='a', header = False, index = False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa4176d-39d1-44ee-b202-fb0bfa512b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model/create pipeline\n",
    "model_name = \"ivnle/llamatales_jr_8b-lay8-hs512-hd8-33M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(\"cuda\")\n",
    "\n",
    "# Prompts\n",
    "prompts = {\n",
    "    1: \"Once upon a time there was a dragon\", \n",
    "    2: \"Once upon a time there was a princess\", \n",
    "    3: \"Once upon a time there were two children\",\n",
    "    4: \"Once upon a time there was a prince\",\n",
    "    5: \"Once upon a time there was a frog\",\n",
    "    6: \"Once upon a time there was a king\",\n",
    "    7: \"Once upon a time there was a queen\",\n",
    "    8: \"Once upon a time there was a wolf\",\n",
    "    9: \"Once upon a time there was a genie\",\n",
    "    10: \"Once upon a time there was a poor boy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a132fda2-9475-4205-863a-4763ef0d904d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts.values():\n",
    "    print(len(tokenizer.encode(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f5ff0-e577-4c08-98a5-0a85f8ada247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     if(len(npz_data[f\"arr_{i}\"][0][0][0][0]) != 512):\n",
    "#         # print(i)\n",
    "#         print(len(npz_data[f\"arr_{i}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e350f-db30-47d5-8108-2c22889d1792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# npz_data[\"arr_1\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74ce8e-5c0d-46ff-8913-b6edff862302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#100x9x1x9x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03cda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"story_dataset_new.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
